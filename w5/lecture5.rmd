---
title: "Lecture 5"
author: "Michal Kubi&#353;ta"
date: "5 February 2018"
output:
  ioslides_presentation:
    widescreen: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, ffmpeg.format = "mp4",
                      cache = TRUE)
library(magrittr)
library(ggplot2)
library(gganimate)
# library(gridExtra)
```

## Structure
1. Regression
2. Pricing

# Regression

##  {.flexbox .vcenter}

<font size="24" color="orange"> **You tell me...**  
recall you statistics / econometrics class </font>

## Econometric approach
OLS

- statistical method
- assumptions (Gauss-Markov)
    1. linear relationship
        - transformations
    2. zero expected error ($E(\epsilon_{i}) = 0$)
        - intercept
    3. no perfect multicollinearity (full X matrix rank)
    4. <font color="orange">**homoskedasticity**</font> ($E(\epsilon_{i}^{2}) = const$)
        - slope
        
## OLS estimation
$$\hat\beta = (X^{T}X)^{-1}X^{T}y$$
**n** observations, **k** variables (\* = OK for small **k**)

- $X^T \; complextity \; O(nk)$ *
- $X^{T}X = T_{k*k} \; complexity \; O(n^{2})$ <font color="red">**!**</font>
- $T^{-1} = U_{k*k} \; complexity \; O(k^{3})$ *
- $UX^{T} = V_{k*n} \; complexity \; O(k^{2}n)$ *
- $Vy = W_{k*1} \; complexity \; O(kn)$ *

>- what if **k** is not small?


## Setup the estimation of the linear relationship
 how to uniquely describe a line (2D) ?
 
>- 2 parameters, slope and a point
>- there is only 1 parameter to estimate - why?
>- $(\bar X,\bar Y)$
>- optimisation task

## (i) random guess

```{r rndAnim, fig.show='animate'}
rand <- list()

xhat <- mean(mtcars$drat)
yhat <- mean(mtcars$wt)
xmin <- min(mtcars$drat)
xmax <- max(mtcars$drat)

for(i in 1:10){
  slope <- runif(1,-10,10)
  ymax <- yhat + slope*(xmax - xhat)
  ymin <- yhat + slope*(xmin - xhat)
  rand[[i]] <- rbind(c(xmin, ymin, i),c(xmax, ymax, i))
}

rand <- do.call(rbind.data.frame, rand)
colnames(rand) <- c("x","y","iter")

rndPlot <-
  ggplot(rand, aes(x = x, y = y, frame = iter))+
  geom_line()+
  geom_point(data = mtcars, aes(x = drat, y = wt), inherit.aes = F)

gganimate(rndPlot)
```

## (ii) random guess + evolution

- save the parameters and SSR of first iteration - best estimation
- if other parameters lead to lower SSR
    - rewrite the best estimation
- new estimation close to the parameters of the best estimation 
- <font color="orange">**very simple**</font> MCMC process
  
## (iii) random guess + evolution + reproduction

1. generate many random "estimates" of slope
2. find the best individuals
    - less then threshold SSR
    - or given number of best-fit individuals
3. let them breed
    - averaging, linear combinations
4. find the best individuals of the extended population
5. repeat 3. and 4. until convergence / stopping condition
- simple evolution algorithm

## Evolution estimation

```{r evoAnim, fig.show='animate'}
xhat <- mean(mtcars$drat)
yhat <- mean(mtcars$wt)
xmin <- rep(min(mtcars$drat), 4)
xmax <- rep(max(mtcars$drat), 4)

evo <- list()

for(i in 1:10){
  if(i == 1){
    slope <- runif(10,-10,10)
  } else{
    slope <- c(slope, (slope[1]+slope[-1])/2)
  }
  
  SSR <- c()
  for(j in slope){
    SSR <- c(SSR, sum((mtcars$wt - (mtcars$drat - xhat) * j - yhat)^2))
  }
  
  parents <- order(SSR)[1:4]
  
  slope <- slope[parents]
  
  ymax <- yhat + slope*(xmax - xhat)
  ymin <- yhat + slope*(xmin - xhat)
  iter <- i
  col <- rep(1:4, each = 2)
  
  evo[[i]] <- cbind(c(rbind(xmin,xmax)), c(rbind(ymin, ymax)), iter, col)
  
}
evo <- do.call(rbind.data.frame, evo)
colnames(evo) <- c("x", "y", "iter", "col")


evoPlot <-
  ggplot(evo, aes(x = x, y = y, frame = iter, col = as.factor(col)))+
  geom_line()+
  geom_point(data = mtcars, aes(x = drat, y = wt), inherit.aes = F)+
  theme(legend.position = 'none')

gganimate(evoPlot)
```

## (iv) gradient descent
1. calculate the gradient of the function
    - $SSR = \sum_i (y_{i} - \hat y_{i})^{2} =
    \sum_i (y_{i} - (a + bx_{i}))^{2} =$  
    $\sum_i (y_{i} - (\hat y - b \hat x) - bx_{i})^{2} = \sum_i (y_{i} - \hat y - b(x_{i} - \hat x))^{2}$
    - $\frac{\nabla SSR}{n} = \frac{\partial SSR}{\partial b} * \frac{1}{n} = H(b)$
2. generate initial random "estimate" of slope = $b_{0}$
3. move a little in the direction of the gradient
    - $b_{n} = b_{n-1} - \alpha H(b)$
    - $\alpha = parameter$
4. repeat 3. until convergence / stopping condition

## Differences
- for the purpose of fitting the linear relationship (no inference)
    - no assumptions
- for inference (prediction)
    - mostly care about homoskedasticity & outliers
       - different error function
    - cross-validation
- watch out for local optimas!
    - ABC optimisation

# Pricing

## Based on product type
- new products (price setting)
    - refer to any marketing introduction book
        - surveys, focus groups
        - market elasticity, competition prices, pricing objectives, ...
    - choosing a similar product as benchmark
- existing products (price corrections)
    - different products (manufacturers)
        - similar to new products
        - benchmarking against similar products
    - same products (retailers, wholesalers)
        - main focus today
            - enough data properly analyse

## Important concepts
- elasticity
- substitutes & complements
    - cross-elasticity

<font size="24" color="orange"> recall you microeconomics class </font>

## Log-log linear model & elasticities

from elasticity to log-log model  

$E = \frac{\frac{\partial Q}{Q}}
            {\frac{\partial P}{P}} =
     \frac{\partial Q}{\partial P}\frac{P}{Q} \Rightarrow
     \frac{\partial Q}{Q} = E\frac{\partial P}{P}$  
$\ln Q = E \ln P + c \Rightarrow \ln Q = \alpha + \beta \ln P$  
  
</br>  
similarly for cross-elasticities

$ln Q = \alpha + \beta_{0} \ln P_{0} + \beta_{1} \ln P_{1} + \cdots + \beta_{n} \ln P_{n}$  
- $\beta_{j} > 0 \Rightarrow$ substitutes, $j > 0$  
- $\beta_{j} < 0 \Rightarrow$ complements, $j > 0$

## Much ado about elasticities?
- elasticity coefficient
    - revenues development in price change
    - determines price change desirability
    - insufficient on it's own
- adding price index
    - product relative price
    - determines the price change direction
- together:
    - where I want to move the price
    - is it desirable
    
## PIE matrix
- not a technical term
- PriceIndex-Elasticity matrix
```{r PIE_sample, fig.height = 4, fig.align='center'}
set.seed(1438)
pieMat <- data.frame(id = c(1:20), pi = sample(1:3,20,TRUE), e = sample(1:3,20,TRUE))

ggplot(pieMat, aes(x = pi, y = e, col = as.factor(id)))+
  geom_jitter(width = 0.15, height = 0.15)+
  scale_y_continuous(name = "elasticity", breaks = 0.5:3.5, limits = c(0.63,3.37))+
  scale_x_continuous(name = "price index", breaks = 0.5:3.5, limits = c(0.63,3.37))+
  theme(panel.grid.major = element_line(color = "black"),
        legend.position = "none",
        axis.text = element_blank(),
        axis.ticks = element_blank())




```

## PIE matrix interpretation
```{r PIE_explained, fig.align='center', fig.width = 8, fig.height = 5}
ggplot(pieMat, aes(x = pi, y = e, col = as.factor(id)))+
  geom_jitter(width = 0.15, height = 0.15)+
  scale_y_continuous(name = "elasticity", breaks = 0.5:3.5, limits = c(0.63,3.37))+
  scale_x_continuous(name = "price index", breaks = 0.5:3.5, limits = c(0.63,3.37))+
  geom_rect(xmin=0.5, xmax = 1.5, ymin = 0.5, ymax = 1.5,
            color= "#E99042", alpha = 0, size = 1.5)+
  geom_rect(xmin = 2.5, xmax = 3.5, ymin = 2.5, ymax = 3.5,
            color = "#E99042", alpha = 0, size = 1.5)+
  annotate("segment", x = 1.3, xend= 1.7, y = 1, yend = 1,
           size = 1.2, color = "red2", arrow = arrow())+
  annotate("segment", x = 2.7, xend= 2.3, y = 3, yend = 3,
           size = 1.2, color = "red2", arrow = arrow())+
  theme(panel.grid.major = element_line(color = "black"),
        legend.position = "none",
        axis.text = element_blank(),
        axis.ticks = element_blank())
```

## What is missing?
- the target price is set
- how to achieve it
- how about promotions / discounts
    - everyday low?
    - high-low strategy?
    
>- promo / nopromo elasticity

## PENE matrix
- again, no technical term
- PromoElasticity-NopromoElasticity
```{r PENE_sample, fig.height = 4, fig.align='center'}
peneMat <- data.frame(id = c(1:20), ep = c(rep(1,8), rep(2,12)), en = c(rep(2,12), rep(1,8)))

ggplot(peneMat, aes(x = ep, y = en, col = as.factor(id)))+
  geom_jitter(width = 0.15, height = 0.15)+
  scale_y_continuous(name = "nopromo",breaks = 0.5:2.5, limits = c(0.63,2.37))+
  scale_x_continuous(name = "promo", breaks = 0.5:2.5, limits = c(0.63,2.37))+
  theme(panel.grid.major = element_line(color = "black"),
        legend.position = "none",
        axis.text = element_blank(),
        axis.ticks = element_blank())
```

## PENE matrix interpretation
```{r PENE_explained, fig.align='center', fig.width = 8, fig.height = 5}
ggplot(peneMat, aes(x = ep, y = en, col = as.factor(id)))+
  geom_jitter(width = 0.15, height = 0.15)+
  scale_y_continuous(name = "nopromo",breaks = 0.5:2.5, limits = c(0.63,2.37))+
  scale_x_continuous(name = "promo", breaks = 0.5:2.5, limits = c(0.63,2.37))+
  annotate("text", x = 2 , y = 1.7, label = "both", size = 5)+
  annotate("text", x = 1.0 , y = 1.7, label = "everyday low", size = 5)+
  annotate("text", x = 2 , y = 1.3, label = "high-low", size = 5)+
  theme(panel.grid.major = element_line(color = "black"),
        legend.position = "none",
        axis.text = element_blank(),
        axis.ticks = element_blank())
```

## Cross elasticities
- high-dimensional data
    - for $k$ products we have $k^{2}$ coefficients
    - needs to be controlled (LASSO)
- could be used as secondary source
    - optimising a category
    - constructing a cross-category bundles

## Back to model estimation
```{r lmer_sample, fig.align='center', fig.width = 8, fig.height = 5}
x <- 1 : 9
y <- c(3, 2, 1, 5, 4, 3, 7, 6, 5)
z = rep(1:3,each = 3)
mixed <- data.frame(x, y, z)

ggplot(mixed, aes(x = x, y = y))+
  geom_point(size = 3)+
  geom_smooth(method="lm")+
  scale_x_continuous(name = "price")+
  scale_y_continuous(name = "quantity")+
  theme(axis.text = element_blank(),
        axis.ticks = element_blank())+
  annotate("text", x = 4.5, y = 7, label = paste("slope = ",lm(y~x)$coef[2]))
  
```

## How about segmentation?
```{r lmer_groups, fig.align='center', fig.width = 8, fig.height = 5}
ggplot(mixed, aes(x = x, y = y, col = as.factor(z)))+
  geom_point(size = 3, color = "black")+
  geom_smooth(method="lm")+
  scale_x_continuous(name = "price")+
  scale_y_continuous(name = "quantity")+
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        legend.position = "none")
```

## Possible segments
- location and store
    - urban / rural
    - town size
    - channel
    - distance to competition
- product
    - category
    - manufacturer
- customers
    - price segmentation
    - promo segmentation
</div>

## Segmentation in model
- dummy-variables
    - intercept only
- large number of models
    - for each group
- mixed effect models
    - estimating overall (fixed) and group (random) effects
    
## Mixed effect models
- avoids the curse of averages
    - the average of $-1$ and $+1$ is $0$
- $y = X\beta + Z \mu + \epsilon$
- maximum likelihood estimation
- better than group of simple lm:
    - data pooling - less data demanding
    - shrinking towards overall average
    - single framework
- classical implementation - lme4::lmer 
    
## Solving segmentation by mixed effect model
```{r}
mm <- lme4::lmer(y ~ x + (1 + x|as.factor(z)), mixed)

efR <- coef(mm)[[1]]

ggplot(mixed, aes(x = x, y = y, col = as.factor(z)))+
  geom_point()+
  geom_abline(intercept = efR[1,1], slope = efR[1,2], col = "#F8766D")+
  geom_abline(intercept = efR[2,1], slope = efR[2,2], col = "#00BA38")+
  geom_abline(intercept = efR[3,1], slope = efR[3,2], col = "#619CFF")+
  scale_x_continuous(name = "price")+
  scale_y_continuous(name = "quantity")+
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        legend.position = "none")
```

## Mixed versus simple regression
- mixed effects
    - very computionally intensive
    - harder to interpret
    - solving averaging problem
- fixed effect
    - simpler
        - computation and interpretation
    - enough for marginal effects
    

